{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee0aae7a",
   "metadata": {},
   "source": [
    "# Binary Classification of Neutral vs Emotional States Using BVP Signals\n",
    "\n",
    "This notebook implements a binary classification model to distinguish between neutral and emotional states using Blood Volume Pulse (BVP) signals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999d0b7c",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Import libraries such as NumPy, pandas, matplotlib, scikit-learn, and any other necessary libraries for signal processing and classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, classification_report,\n",
    "                             roc_curve, auc, roc_auc_score)\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22951c94",
   "metadata": {},
   "source": [
    "## Load and Explore Dataset\n",
    "\n",
    "Load the dataset containing BVP signals and explore its structure, including the distribution of classes and signal characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e854e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Note: Update the file path according to your dataset location\n",
    "data_path = r'e:\\Final Year Project\\MyCodeSpace\\Current(2026-02-21)\\bvp_data.csv'\n",
    "\n",
    "# Read the dataset\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b2e60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore class distribution\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "df['label'].value_counts().plot(kind='bar', color=['skyblue', 'salmon'])\n",
    "plt.title('Original Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df['label'].value_counts().plot(kind='pie', autopct='%1.1f%%', colors=['skyblue', 'salmon'])\n",
    "plt.title('Class Distribution (%)')\n",
    "plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f05581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample BVP signals from different classes\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Get sample signals for each class\n",
    "classes = df['label'].unique()\n",
    "for idx, cls in enumerate(classes[:2]):  # Show first 2 classes\n",
    "    sample_signal = df[df['label'] == cls].iloc[0]\n",
    "    signal_data = sample_signal.drop('label').values\n",
    "    \n",
    "    plt.subplot(1, 2, idx + 1)\n",
    "    plt.plot(signal_data)\n",
    "    plt.title(f'Sample BVP Signal - Class: {cls}')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('BVP Amplitude')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f792dbd",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "Clean the data by handling missing values, normalizing the BVP signals, and applying any necessary filtering techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5856c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Handle missing values if any\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    print(\"\\nHandling missing values...\")\n",
    "    df = df.dropna()  # or use df.fillna() with appropriate strategy\n",
    "    print(\"Missing values after handling:\", df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad34573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply bandpass filter to BVP signals\n",
    "def apply_bandpass_filter(signal_data, lowcut=0.5, highcut=4.0, fs=64, order=4):\n",
    "    \"\"\"\n",
    "    Apply bandpass filter to BVP signal\n",
    "    Typical BVP frequency range: 0.5-4.0 Hz (30-240 BPM)\n",
    "    \"\"\"\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = signal.butter(order, [low, high], btype='band')\n",
    "    filtered_signal = signal.filtfilt(b, a, signal_data)\n",
    "    return filtered_signal\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop('label', axis=1).values\n",
    "y = df['label'].values\n",
    "\n",
    "# Apply filtering to each signal\n",
    "print(\"Applying bandpass filter to BVP signals...\")\n",
    "X_filtered = np.array([apply_bandpass_filter(signal_data) for signal_data in X])\n",
    "\n",
    "print(\"Filtering completed!\")\n",
    "print(\"Filtered data shape:\", X_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33333f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize effect of filtering\n",
    "sample_idx = 0\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(X[sample_idx])\n",
    "plt.title('Original BVP Signal')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(X_filtered[sample_idx])\n",
    "plt.title('Filtered BVP Signal')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311aa9b7",
   "metadata": {},
   "source": [
    "## Balance Classes\n",
    "\n",
    "Use undersampling to balance the neutral class with other emotional classes, ensuring a balanced dataset for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001440da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary labels: Neutral (0) vs Emotional (1)\n",
    "# Assuming 'neutral' is one specific class and all others are emotional\n",
    "print(\"Original unique labels:\", np.unique(y))\n",
    "\n",
    "# Define neutral and emotional classes\n",
    "# Adjust this based on your dataset's labeling scheme\n",
    "neutral_label = 'neutral'  # or 0, depending on your dataset\n",
    "\n",
    "# Create binary labels\n",
    "y_binary = np.where(y == neutral_label, 0, 1)\n",
    "\n",
    "print(\"\\nBinary Class Distribution:\")\n",
    "unique, counts = np.unique(y_binary, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    class_name = \"Neutral\" if label == 0 else \"Emotional\"\n",
    "    print(f\"{class_name} (Class {label}): {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f3a453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply undersampling to balance classes\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Combine features and labels\n",
    "df_balanced = pd.DataFrame(X_filtered)\n",
    "df_balanced['label'] = y_binary\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = df_balanced[df_balanced['label'] == 0]\n",
    "df_minority = df_balanced[df_balanced['label'] == 1]\n",
    "\n",
    "# Determine which is majority and minority\n",
    "if len(df_majority) < len(df_minority):\n",
    "    df_majority, df_minority = df_minority, df_majority\n",
    "\n",
    "print(f\"Majority class size: {len(df_majority)}\")\n",
    "print(f\"Minority class size: {len(df_minority)}\")\n",
    "\n",
    "# Undersample majority class\n",
    "df_majority_downsampled = resample(df_majority,\n",
    "                                   replace=False,\n",
    "                                   n_samples=len(df_minority),\n",
    "                                   random_state=42)\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "df_balanced = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "# Shuffle the dataset\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nBalanced dataset size: {len(df_balanced)}\")\n",
    "print(\"\\nBalanced Class Distribution:\")\n",
    "print(df_balanced['label'].value_counts())\n",
    "\n",
    "# Separate features and labels again\n",
    "X_balanced = df_balanced.drop('label', axis=1).values\n",
    "y_balanced = df_balanced['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee0e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize balanced class distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "pd.Series(y_binary).value_counts().plot(kind='bar', color=['skyblue', 'salmon'])\n",
    "plt.title('Before Balancing')\n",
    "plt.xlabel('Class (0=Neutral, 1=Emotional)')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "pd.Series(y_balanced).value_counts().plot(kind='bar', color=['skyblue', 'salmon'])\n",
    "plt.title('After Balancing')\n",
    "plt.xlabel('Class (0=Neutral, 1=Emotional)')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80c2f87",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Extract relevant features from the BVP signals, such as time-domain and frequency-domain features, to use as input for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eefae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature extraction functions\n",
    "def extract_time_domain_features(signal_data):\n",
    "    \"\"\"Extract time-domain features from BVP signal\"\"\"\n",
    "    features = {\n",
    "        'mean': np.mean(signal_data),\n",
    "        'std': np.std(signal_data),\n",
    "        'min': np.min(signal_data),\n",
    "        'max': np.max(signal_data),\n",
    "        'range': np.max(signal_data) - np.min(signal_data),\n",
    "        'median': np.median(signal_data),\n",
    "        'skewness': skew(signal_data),\n",
    "        'kurtosis': kurtosis(signal_data),\n",
    "        'rms': np.sqrt(np.mean(signal_data**2)),\n",
    "        'peak_to_peak': np.ptp(signal_data)\n",
    "    }\n",
    "    return features\n",
    "\n",
    "def extract_frequency_domain_features(signal_data, fs=64):\n",
    "    \"\"\"Extract frequency-domain features from BVP signal\"\"\"\n",
    "    # Compute power spectral density\n",
    "    freqs, psd = signal.welch(signal_data, fs=fs, nperseg=min(256, len(signal_data)))\n",
    "    \n",
    "    features = {\n",
    "        'spectral_mean': np.mean(psd),\n",
    "        'spectral_std': np.std(psd),\n",
    "        'spectral_max': np.max(psd),\n",
    "        'dominant_frequency': freqs[np.argmax(psd)],\n",
    "        'spectral_energy': np.sum(psd)\n",
    "    }\n",
    "    return features\n",
    "\n",
    "def extract_hrv_features(signal_data, fs=64):\n",
    "    \"\"\"Extract heart rate variability features\"\"\"\n",
    "    # Detect peaks (simplified peak detection)\n",
    "    peaks, _ = signal.find_peaks(signal_data, distance=fs//2)\n",
    "    \n",
    "    if len(peaks) < 2:\n",
    "        return {'hrv_mean': 0, 'hrv_std': 0, 'hrv_rmssd': 0}\n",
    "    \n",
    "    # Calculate RR intervals\n",
    "    rr_intervals = np.diff(peaks) / fs * 1000  # in milliseconds\n",
    "    \n",
    "    features = {\n",
    "        'hrv_mean': np.mean(rr_intervals),\n",
    "        'hrv_std': np.std(rr_intervals),\n",
    "        'hrv_rmssd': np.sqrt(np.mean(np.diff(rr_intervals)**2))\n",
    "    }\n",
    "    return features\n",
    "\n",
    "print(\"Feature extraction functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116140dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from all signals\n",
    "print(\"Extracting features from BVP signals...\")\n",
    "\n",
    "all_features = []\n",
    "for signal_data in X_balanced:\n",
    "    time_features = extract_time_domain_features(signal_data)\n",
    "    freq_features = extract_frequency_domain_features(signal_data)\n",
    "    hrv_features = extract_hrv_features(signal_data)\n",
    "    \n",
    "    # Combine all features\n",
    "    combined_features = {**time_features, **freq_features, **hrv_features}\n",
    "    all_features.append(combined_features)\n",
    "\n",
    "# Convert to DataFrame\n",
    "X_features = pd.DataFrame(all_features)\n",
    "\n",
    "print(\"Feature extraction completed!\")\n",
    "print(f\"Feature matrix shape: {X_features.shape}\")\n",
    "print(\"\\nExtracted features:\")\n",
    "print(X_features.columns.tolist())\n",
    "print(\"\\nFeature statistics:\")\n",
    "print(X_features.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11f4661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature correlations\n",
    "plt.figure(figsize=(14, 10))\n",
    "correlation_matrix = X_features.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            square=True, linewidths=0.5)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccc665c",
   "metadata": {},
   "source": [
    "## Split Data into Training and Testing Sets\n",
    "\n",
    "Split the dataset into training and testing sets, ensuring that the split is stratified to maintain class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf354ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_features, y_balanced, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_balanced\n",
    ")\n",
    "\n",
    "print(\"Data split completed!\")\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "print(f\"\\nTesting set class distribution:\")\n",
    "print(pd.Series(y_test).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825e2478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature normalization completed!\")\n",
    "print(f\"Scaled training data shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled testing data shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d176dc2",
   "metadata": {},
   "source": [
    "## Train Binary Classification Model\n",
    "\n",
    "Train a binary classification model (e.g., logistic regression, SVM, or a neural network) to classify neutral vs emotional states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac1c54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize multiple models for comparison\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'SVM (Linear)': SVC(kernel='linear', random_state=42, probability=True),\n",
    "    'SVM (RBF)': SVC(kernel='rbf', random_state=42, probability=True),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Train all models\n",
    "trained_models = {}\n",
    "print(\"Training models...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # Calculate training accuracy\n",
    "    train_acc = model.score(X_train_scaled, y_train)\n",
    "    print(f\"{name} - Training Accuracy: {train_acc:.4f}\")\n",
    "    \n",
    "print(\"\\nAll models trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa0427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation for each model\n",
    "print(\"\\nCross-Validation Results (5-fold):\\n\")\n",
    "\n",
    "cv_results = {}\n",
    "for name, model in trained_models.items():\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "    cv_results[name] = cv_scores\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Mean CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    print(f\"  Individual Fold Scores: {cv_scores}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c51876",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance\n",
    "\n",
    "Evaluate the model's performance using metrics such as accuracy, precision, recall, F1-score, and a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c491577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models on test set\n",
    "print(\"Model Evaluation on Test Set:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "evaluation_results = {}\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    evaluation_results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1:.4f}\")\n",
    "    print(f\"  ROC-AUC:   {roc_auc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22a52f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "results_df = pd.DataFrame(evaluation_results).T\n",
    "results_df = results_df[['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']]\n",
    "\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(results_df)\n",
    "\n",
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Bar plot\n",
    "results_df.plot(kind='bar', ax=axes[0], rot=45)\n",
    "axes[0].set_title('Model Performance Comparison')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(results_df.T, annot=True, fmt='.3f', cmap='YlGnBu', ax=axes[1], \n",
    "            cbar_kws={'label': 'Score'})\n",
    "axes[1].set_title('Model Performance Heatmap')\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].set_ylabel('Metric')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3e1595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrices for all models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, model) in enumerate(trained_models.items()):\n",
    "    y_pred = evaluation_results[name]['y_pred']\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=['Neutral', 'Emotional'],\n",
    "                yticklabels=['Neutral', 'Emotional'])\n",
    "    axes[idx].set_title(f'Confusion Matrix - {name}')\n",
    "    axes[idx].set_ylabel('True Label')\n",
    "    axes[idx].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1288c8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed classification reports\n",
    "print(\"\\nDetailed Classification Reports:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    y_pred = evaluation_results[name]['y_pred']\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(classification_report(y_test, y_pred, \n",
    "                                target_names=['Neutral', 'Emotional'],\n",
    "                                digits=4))\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe2fd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    y_pred_proba = evaluation_results[name]['y_pred_proba']\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = evaluation_results[name]['roc_auc']\n",
    "    \n",
    "    plt.plot(fpr, tpr, linewidth=2, label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier (AUC = 0.500)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves - Binary Classification (Neutral vs Emotional)', fontsize=14)\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47df8e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on F1-score\n",
    "best_model_name = results_df['f1_score'].idxmax()\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"F1-Score: {results_df.loc[best_model_name, 'f1_score']:.4f}\")\n",
    "print(f\"\\nBest Model Parameters:\")\n",
    "print(best_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5246bbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (for Random Forest)\n",
    "if 'Random Forest' in trained_models:\n",
    "    rf_model = trained_models['Random Forest']\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_features.columns,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features (Random Forest):\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(feature_importance['feature'][:10], feature_importance['importance'][:10])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Top 10 Feature Importances - Random Forest')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26e8bd4",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "Summary of the binary classification results for neutral vs emotional states using BVP signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa3dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY - BINARY CLASSIFICATION (NEUTRAL VS EMOTIONAL)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nDataset Information:\")\n",
    "print(f\"  Total samples after balancing: {len(X_balanced)}\")\n",
    "print(f\"  Number of features extracted: {X_features.shape[1]}\")\n",
    "print(f\"  Training samples: {len(X_train)}\")\n",
    "print(f\"  Testing samples: {len(X_test)}\")\n",
    "\n",
    "print(f\"\\nBest Performing Model: {best_model_name}\")\n",
    "print(f\"  Accuracy:  {results_df.loc[best_model_name, 'accuracy']:.4f}\")\n",
    "print(f\"  Precision: {results_df.loc[best_model_name, 'precision']:.4f}\")\n",
    "print(f\"  Recall:    {results_df.loc[best_model_name, 'recall']:.4f}\")\n",
    "print(f\"  F1-Score:  {results_df.loc[best_model_name, 'f1_score']:.4f}\")\n",
    "print(f\"  ROC-AUC:   {results_df.loc[best_model_name, 'roc_auc']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nClassification task completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f544148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY - BINARY CLASSIFICATION (NEUTRAL VS EMOTIONAL)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nDataset Information:\")\n",
    "print(f\"  Total samples after balancing: {len(X_balanced)}\")\n",
    "print(f\"  Number of features extracted: {X_features.shape[1]}\")\n",
    "print(f\"  Training samples: {len(X_train)}\")\n",
    "print(f\"  Testing samples: {len(X_test)}\")\n",
    "\n",
    "print(f\"\\nBest Performing Model: {best_model_name}\")\n",
    "print(f\"  Accuracy:  {results_df.loc[best_model_name, 'accuracy']:.4f}\")\n",
    "print(f\"  Precision: {results_df.loc[best_model_name, 'precision']:.4f}\")\n",
    "print(f\"  Recall:    {results_df.loc[best_model_name, 'recall']:.4f}\")\n",
    "print(f\"  F1-Score:  {results_df.loc[best_model_name, 'f1_score']:.4f}\")\n",
    "print(f\"  ROC-AUC:   {results_df.loc[best_model_name, 'roc_auc']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nClassification task completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
